doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = 1000,
#convergence_tol = 0.001, n_check_convergence = 25,
convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
sink()
perp <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
m_k <- round (x[1])
m_alpha <- x[2]
m_beta <- x[3]
print(paste("k:", m_k, "alpha:", m_alpha, "beta", m_beta, "perp:", perp))
perp
}
# MITÄ enemmän artikkeleita, sitä suurempi HIGHER eka luku. Eli optimointia sen suhteen, että ei ole liikaa eikä liian vähän artikkeleita, jotta trendi näkyis todenmukasesti
lower <- c(10, 0, 0)
higher <- c(150, 1, 0.3)
# TÄTÄ HÄÄTYY SÄÄTÄÄ vissin
DEoptim(optimalLda, lower, higher, DEoptim.control(strategy = 2, itermax = 10, NP = 10, CR = 0.5, F = 0.8))
# TÄMÄ TOIMII JOSKUS
library(DEoptim)
# MITÄ enemmän artikkeleita, sitä suurempi HIGHER eka luku. Eli optimointia sen suhteen, että ei ole liikaa eikä liian vähän artikkeleita, jotta trendi näkyis todenmukasesti
lower <- c(10, 0, 0)
higher <- c(150, 1, 0.3)
# TÄTÄ HÄÄTYY SÄÄTÄÄ vissin
DEoptim(optimalLda, lower, higher, DEoptim.control(strategy = 2, itermax = 10, NP = 10, CR = 0.5, F = 0.8))
# TÄMÄ TOIMII JOSKUS
library(DEoptim)
# MITÄ enemmän artikkeleita, sitä suurempi HIGHER eka luku. Eli optimointia sen suhteen, että ei ole liikaa eikä liian vähän artikkeleita, jotta trendi näkyis todenmukasesti
lower <- c(10, 0, 0)
higher <- c(150, 1, 0.3)
# TÄTÄ HÄÄTYY SÄÄTÄÄ vissin
DEoptim(optimalLda, lower, higher, DEoptim.control(strategy = 2, itermax = 10, NP = 10, CR = 0.5, F = 0.8))
tokens = my_articles$Clean_Text %>%  tokenize_words (strip_numeric = TRUE)
it <- itoken(tokens, progressbar = FALSE)
v = create_vocabulary(it) %>% prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.3)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model = LDA$new(n_topics = 298, doc_topic_prior = 0.2518732, topic_word_prior = 0.005613016)
doc_topic_distr = lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
#convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#Save model for further analysis
lda_file = getwd()
lda_file = paste(lda_file, "/", sep="")
lda_file = paste(lda_file, my_data_dir, sep="")
lda_file = paste(lda_file, "/", sep="")
lda_file_doc_topic_dist = paste(lda_file, "LDADocTopicDist.RData", sep="")
lda_file = paste(lda_file, "LDAModel.RData", sep="")
save(lda_model, file=lda_file)
save(doc_topic_distr, file=lda_file_doc_topic_dist)
#sanasto?
lda_model
#function------------------------------------------------------------
optimalLda <- function (x){
sink("NUL")
m_k <- round (x[1])
m_alpha <- x[2]
m_beta <- x[3]
sample <- sample.int(n = nrow(my_articles), size = floor(.80*nrow(my_articles)), replace = F)
tokens = my_articles$Clean_Text [sample] %>%  tokenize_words (strip_numeric = TRUE)
it <- itoken(tokens, progressbar = FALSE)
v = create_vocabulary(it) %>%
prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.1)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
#Find correct hyper parameters.
lda_model = LDA$new(n_topics = m_k, doc_topic_prior = m_alpha, topic_word_prior = m_beta)
#lda_model = LDA$new(n_topics = 100, doc_topic_prior = 0.1, topic_word_prior = 0.1)
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = 1000,
#convergence_tol = 0.001, n_check_convergence = 25,
convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
sink()
perp <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
m_k <- round (x[1])
m_alpha <- x[2]
m_beta <- x[3]
print(paste("k:", m_k, "alpha:", m_alpha, "beta", m_beta, "perp:", perp))
perp
}
#function------------------------------------------------------------
optimalLda <- function (x){
sink("NUL")
m_k <- round (x[1])
m_alpha <- x[2]
m_beta <- x[3]
sample <- sample.int(n = nrow(my_articles), size = floor(.80*nrow(my_articles)), replace = F)
tokens = my_articles$Clean_Text [sample] %>%  tokenize_words (strip_numeric = TRUE)
it <- itoken(tokens, progressbar = FALSE)
v = create_vocabulary(it) %>%
prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.1)
vectorizer = vocab_vectorizer(v)
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
#Find correct hyper parameters.
lda_model = LDA$new(n_topics = m_k, doc_topic_prior = m_alpha, topic_word_prior = m_beta)
#lda_model = LDA$new(n_topics = 100, doc_topic_prior = 0.1, topic_word_prior = 0.1)
doc_topic_distr =
lda_model$fit_transform(x = dtm, n_iter = 1000,
#convergence_tol = 0.001, n_check_convergence = 25,
convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
sink()
perp <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
m_k <- round (x[1])
m_alpha <- x[2]
m_beta <- x[3]
print(paste("k:", m_k, "alpha:", m_alpha, "beta", m_beta, "perp:", perp))
perp
}
# TÄMÄ TOIMII JOSKUS
library(DEoptim)
# MITÄ enemmän artikkeleita, sitä suurempi HIGHER eka luku. Eli optimointia sen suhteen, että ei ole liikaa eikä liian vähän artikkeleita, jotta trendi näkyis todenmukasesti
lower <- c(10, 0, 0)
higher <- c(150, 1, 0.3)
# TÄTÄ HÄÄTYY SÄÄTÄÄ vissin
DEoptim(optimalLda, lower, higher, DEoptim.control(strategy = 2, itermax = 10, NP = 10, CR = 0.5, F = 0.8))
library(tm)
library(ggplot2)
library(xtable)
library("text2vec")
library("nnet") #Breaks ties at random when searching for max
my_file = "my_Scopus_botnet-sco_data.RData"
#Articles. Make sure this is the same you used to build LDA model otherwise it will not make any sense
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
#LDAWinner
my_LDAWinner_file = paste(my_work_dir,  "/", sep="")
my_LDAWinner_file = paste(my_LDAWinner_file, my_data_dir, sep="")
my_LDAWinner_file = paste(my_LDAWinner_file, "/LDAModel.RData", sep="")
my_doctopicdist_file = paste(my_work_dir,  "/", sep="")
my_doctopicdist_file = paste(my_doctopicdist_file, my_data_dir, sep="")
my_doctopicdist_file = paste(my_doctopicdist_file, "/LDADocTopicDist.RData", sep="")
load(my_doctopicdist_file)
load(my_LDAWinner_file)
#Create important arrays with descriptive names
#Documents to topics and get top 'n' terms for each topic
Topics <- apply(doc_topic_distr, 1, function(x) which.is.max (x))
# Terms = lda_model$get_top_words(LDAWinner, 50)
Terms = lda_model$get_top_words(50)
#Still in box......................................
Titles = my_articles[,"Title"]
Years = my_articles[,"Date"]
Cites = my_articles[, "Cites"]
Abstracts = my_articles[,"Abstract_clean"]
my_articles$Years = as.numeric(format(my_articles$Date, "%Y"))
Years = my_articles$Years
topics_n = lda_model$.__enclos_env__$private$n_topics
#List top ten terms for all topics
Terms[1:10,]
#Study one topic (Replace by the optimal topics)
Terms[1:10,topics_n]
#List all papers for topic 1
my_articles$Title[Topics==1]
my_articles$Abstract[Topics==1]
my_articles$Abstract_clean[Topics==1]
#Search for hot topic
medians = lapply(1:length(Terms[1,]), function(i) median(as.numeric(Years[Topics==i])))
#The "Hottest topic"
Terms[1:10,which.max(medians)]
Titles[Topics==which.max(medians)]
#The coldest topic
Terms[1:10,which.min(medians)]
Titles[Topics==which.min(medians)]
#Density plots
qplot(as.numeric(Years), colour=factor(Topics),  geom="density")
library(tm)
library(ggplot2)
library(xtable)
library("text2vec")
library("nnet") #Breaks ties at random when searching for max
my_file = "my_Scopus_botnet-sco_data.RData"
#Articles. Make sure this is the same you used to build LDA model otherwise it will not make any sense
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
#LDAWinner
my_LDAWinner_file = paste(my_work_dir,  "/", sep="")
my_LDAWinner_file = paste(my_LDAWinner_file, my_data_dir, sep="")
my_LDAWinner_file = paste(my_LDAWinner_file, "/LDAModel.RData", sep="")
my_doctopicdist_file = paste(my_work_dir,  "/", sep="")
my_doctopicdist_file = paste(my_doctopicdist_file, my_data_dir, sep="")
my_doctopicdist_file = paste(my_doctopicdist_file, "/LDADocTopicDist.RData", sep="")
load(my_doctopicdist_file)
load(my_LDAWinner_file)
#Create important arrays with descriptive names
#Documents to topics and get top 'n' terms for each topic
Topics <- apply(doc_topic_distr, 1, function(x) which.is.max (x))
# Terms = lda_model$get_top_words(LDAWinner, 50)
Terms = lda_model$get_top_words(50)
#Still in box......................................
Titles = my_articles[,"Title"]
Years = my_articles[,"Date"]
Cites = my_articles[, "Cites"]
Abstracts = my_articles[,"Abstract_clean"]
my_articles$Years = as.numeric(format(my_articles$Date, "%Y"))
Years = my_articles$Years
topics_n = lda_model$.__enclos_env__$private$n_topics
#List top ten terms for all topics
Terms[1:10,]
#Study one topic (Replace by the optimal topics)
Terms[1:10,topics_n]
#List all papers for topic 1
my_articles$Title[Topics==1]
my_articles$Abstract[Topics==1]
my_articles$Abstract_clean[Topics==1]
#Search for hot topic
medians = lapply(1:length(Terms[1,]), function(i) median(as.numeric(Years[Topics==i])))
#The "Hottest topic"
Terms[1:10,which.max(medians)]
Titles[Topics==which.max(medians)]
#The coldest topic
Terms[1:10,which.min(medians)]
Titles[Topics==which.min(medians)]
#Density plots
qplot(as.numeric(Years), colour=factor(Topics),  geom="density")
#Test Plot hot vs. cold
qplot(as.numeric(subset(Years, Topics==which.max(medians) |  Topics==which.min(medians))), colour=factor(subset(Topics , Topics==which.max(medians) |  Topics==which.min(medians))),  geom="density")
# analyze optimal
# build optimal
# interactivedacluster
#
# thesisr
#** CLEAN YOUR ENVIRONMENT
#** You may want to clear your environment variables when starting a session. Saves from plenty of headache.
rm(list=ls())
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("D:/Git/TrendMining")
my_work_dir = getwd()
my_data_dir = "data"
if (!file.exists(my_data_dir)) {
dir.create(file.path(my_work_dir, my_data_dir))
}
#** STACKOVERFLOW API KEY
#** Set your own StackOverflow API key here (or use the default below)
#** EDIT THE FOLLOWING LINE for your own API key
#api_key = "9raZ36FkYGFHDSNrW)gdsw((" TODO old file name edit out
so_api_key = "Qa0f*SDU36j)7KMJbAu)Nw(("
#** GETOLDTWEETS-JAVA PATH
#** Set path to the directory for "GetOldTweets-java-master"
getoldtweets_path = paste(getwd(),"/GetOldTweets-java-master", sep="")
#** SCOPUS API KEY
#** Set your own Scopus API key here
#** Create an account & create your API key => <your-own-scopus-api-key>
#** https://dev.elsevier.com/user/login
#** Replace the next line with set_api_key("YOUR_SCOPUS_KEY_HERE")
#** EDIT THE FOLLOWING LINE with YOUR OWN Scopus API key
#install.packages("rscopus", dependencies = TRUE)
library("rscopus")
sc_api_key = "5adc613940574a3639f109e6f1a2742a"
# set_api_key("cbf4132ececcb84b58a2aa5244ba7ce7")
#alternatively you may store it a personal file somewhere else.
#source("~/ETiSEngineering/TrendMining-master/SetScopusApiKey.R")
#rm(list=ls())
#install.packages("rstudioapi")
library(rstudioapi)
my_work_dir = rstudioapi::getActiveDocumentContext()$path
#my_work_dir = gsub("\\\\", "/", my_work_dir)
pathParts = strsplit(my_work_dir,'/')[[1]]
my_work_dir = paste(pathParts[1:length(pathParts)-1],sep='', collapse = "/")
setwd(my_work_dir)
libPath = paste(my_work_dir,'/','my_libs', sep="")
if (!file.exists(libPath))
{
dir.create(libPath, showWarnings = FALSE)
}
dataPath = paste(my_work_dir,'/','Data', sep="")
if (!file.exists(dataPath))
{
dir.create(dataPath, showWarnings = FALSE)
}
twitterDataPath = paste(my_work_dir,'/','Data','/','Twitter', sep="")
if (!file.exists(twitterDataPath))
{
dir.create(twitterDataPath, showWarnings = FALSE)
}
stackDataPath = paste(my_work_dir,'/','Data','/','Stack', sep="")
if (!file.exists(stackDataPath))
{
dir.create(stackDataPath, showWarnings = FALSE)
}
scopusDataPath = paste(my_work_dir,'/','Data','/','Scopus', sep="")
if (!file.exists(scopusDataPath))
{
dir.create(scopusDataPath, showWarnings = FALSE)
}
if (libPath %in% .libPaths() == FALSE)
{
.libPaths(c(libPath,.libPaths()))
}
sc_api_key = "cbf4132ececcb84b58a2aa5244ba7ce7"
so_api_key = "EgDQqc14dkQjHIDevXFJ)A(("
tw_api_key = ""
getoldtweets_path = paste(my_work_dir,"/GetOldTweets-java-master", sep="")
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"botnet", "botnets")
#install.packages("tm", dependencies = TRUE)
#install.packages("NLP", dependencies = TRUE)
#install.packages("magrittr", dependencies = TRUE)
#install.packages("slam", dependencies = TRUE)
#install.packages("Rmpfr", dependencies = TRUE)
library(tm)
library(NLP)
library("magrittr")
library("text2vec")
library("tokenizers")
# LAITA TÄHÄN oma filunimi
# my_file = "my_Scopus_botnet-sco_data.RData"
my_file = "my_Scopus_botnet-sco_data.RData"
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
# TÄMÄ ON LAITETTAVISSA clasroomsettings filussa
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"myStopword1", "myStopword2")
#Articles with NA dates cause false analysis later kick them out
my_articles <- my_articles[which(!is.na(my_articles$Date)),]
my_text <- paste (my_articles$Title, my_articles$Abstract_clean)
removeSpecialChars <- function(x) gsub("[^a-zA-Z ]","",x)
my_text <- removeSpecialChars(my_text)
my_text <- removeWords(my_text, my_stopwords)
my_articles$Clean_Text <- my_text
#-----------------------------------------------------------------------------------
#Create first LDA model. We select 80% for model creation remaining is for testing
#See tutorial for more details http://text2vec.org/topic_modeling.html#latent_dirichlet_allocation
#model goodness
sample <- sample.int(n = nrow(my_articles), size = floor(.80*nrow(my_articles)), replace = F)
#create tokens
tokens = my_articles$Clean_Text [sample] %>%  tokenize_words (strip_numeric = TRUE)
it <- itoken(tokens, progressbar = FALSE)
#Here we create the vocabulary. Term must appear in min 10 documents (you might want to edit this)
#If term appears in more than 30% documents we remove because it is too frequent (you might want to edit this as well)
v = create_vocabulary(it) %>% prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.3)
vectorizer = vocab_vectorizer(v)
#create document-term matrix
dtm = create_dtm(it, vectorizer, type = "dgTMatrix")
# we create 10 topics
# n_topics VAIKUTTAA VISSIN, VOI SÄÄTÄÄ
lda_model = LDA$new(n_topics = 30, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
#convergence_tol = 0.01, n_check_convergence = 25,
progressbar = FALSE, verbose=FALSE)
#apply to training set
new_dtm = itoken(my_articles$Clean_Text[-sample], tolower, word_tokenizer) %>%
create_dtm(vectorizer, type = "dgTMatrix")
new_doc_topic_distr = lda_model$transform(new_dtm)
perpperplexity_score <- perplexity(new_dtm, topic_word_distribution = lda_model$topic_word_distribution, doc_topic_distribution = new_doc_topic_distr)
#how good is our model
#Try playin with n_topics, doc_topic_prior, topic_word_prior to see how to get better
perpperplexity_score
#how good is our model
#Try playin with n_topics, doc_topic_prior, topic_word_prior to see how to get better
perpperplexity_score
# analyze optimal
# build optimal
# interactivedacluster
#
# thesisr
#** CLEAN YOUR ENVIRONMENT
#** You may want to clear your environment variables when starting a session. Saves from plenty of headache.
rm(list=ls())
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("D:/Git/TrendMining")
my_work_dir = getwd()
my_data_dir = "data"
if (!file.exists(my_data_dir)) {
dir.create(file.path(my_work_dir, my_data_dir))
}
#** STACKOVERFLOW API KEY
#** Set your own StackOverflow API key here (or use the default below)
#** EDIT THE FOLLOWING LINE for your own API key
#api_key = "9raZ36FkYGFHDSNrW)gdsw((" TODO old file name edit out
so_api_key = "Qa0f*SDU36j)7KMJbAu)Nw(("
#** GETOLDTWEETS-JAVA PATH
#** Set path to the directory for "GetOldTweets-java-master"
getoldtweets_path = paste(getwd(),"/GetOldTweets-java-master", sep="")
#** SCOPUS API KEY
#** Set your own Scopus API key here
#** Create an account & create your API key => <your-own-scopus-api-key>
#** https://dev.elsevier.com/user/login
#** Replace the next line with set_api_key("YOUR_SCOPUS_KEY_HERE")
#** EDIT THE FOLLOWING LINE with YOUR OWN Scopus API key
#install.packages("rscopus", dependencies = TRUE)
library("rscopus")
sc_api_key = "5adc613940574a3639f109e6f1a2742a"
# set_api_key("cbf4132ececcb84b58a2aa5244ba7ce7")
#alternatively you may store it a personal file somewhere else.
#source("~/ETiSEngineering/TrendMining-master/SetScopusApiKey.R")
#install.packages("rstudioapi")
library(rstudioapi)
my_work_dir = rstudioapi::getActiveDocumentContext()$path
pathParts = strsplit(my_work_dir,'/')[[1]]
my_work_dir = paste(pathParts[1:length(pathParts)-1],sep='', collapse = "/")
setwd(my_work_dir)
libPath = paste(my_work_dir,'/','my_libs', sep="")
if (!file.exists(libPath))
{
dir.create(libPath, showWarnings = FALSE)
}
dataPath = paste(my_work_dir,'/','Data', sep="")
if (!file.exists(dataPath))
{
dir.create(dataPath, showWarnings = FALSE)
}
twitterDataPath = paste(my_work_dir,'/','Data','/','Twitter', sep="")
if (!file.exists(twitterDataPath))
{
dir.create(twitterDataPath, showWarnings = FALSE)
}
stackDataPath = paste(my_work_dir,'/','Data','/','Stack', sep="")
if (!file.exists(stackDataPath))
{
dir.create(stackDataPath, showWarnings = FALSE)
}
scopusDataPath = paste(my_work_dir,'/','Data','/','Scopus', sep="")
if (!file.exists(scopusDataPath))
{
dir.create(scopusDataPath, showWarnings = FALSE)
}
if (libPath %in% .libPaths() == FALSE)
{
.libPaths(c(libPath,.libPaths()))
}
getoldtweets_path = paste(my_work_dir,"/GetOldTweets-java-master", sep="")
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"botnet", "botnets")
#install.packages("text2vec", dependencies = TRUE)
library("text2vec")
source("FunctionsScopusApi.R")
set_api_key("cbf4132ececcb84b58a2aa5244ba7ce7")
# my_query_string = "botnet"
query_string = "botnet"
my_filename = "botnet-sco"
my_query_string = "TITLE-ABS-KEY(\""
my_query_string = paste(my_query_string, query_string, sep="")
#EDIT this line
#  my_query_string = paste(my_query_string, "\") AND ALL('software testing')", sep="")
my_query_string = paste(my_query_string, "\") AND ALL('software')", sep="")
#Get articles and save those - we do not want to re-run the query
my_articles = get_scopus_papers(my_query_string)
#Remove copyright sign.
abstract = my_articles$Abstract
abstract = gsub("Copyright ?+[^.]*[.]","",abstract)
abstract = gsub("?+[^.]*[.]","",abstract) # Depdenging on the enviroment or data you might need something different*
#Remove copyright sign.
abstract = my_articles$Abstract
abstract = gsub("Copyright ?+[^.]*[.]","",abstract)
View(my_articles)
#Remove copyright sign.
abstract = my_articles$Abstract
abstract = gsub("Copyright ?+[^.]*[.]","",abstract)
#Remove copyright sign.
abstract = my_articles$Abstract
# abstract = gsub("Copyright +[^.]*[.]","",abstract)
# abstract = gsub("?+[^.]*[.]","",abstract) # Depdenging on the enviroment or data you might need something different*
abstract = gsub("Copyright ©+[^.]*[.]","",abstract)
abstract = gsub("©+[^.]*[.]","",abstract) # Depdenging on the enviroment or data you might need something different*
abstract = gsub("All rights reserved[.]","",abstract)
abstract = gsub("All right reserved[.]","",abstract)
abstract = gsub("No abstract available[.]","",abstract)
abstract = gsub("[0-9]", "", abstract)
#It is easy to accidentally too much or too little.
#Check length of abstracts -> ratio of new vs origal
nchar(abstract)/nchar(my_articles$Abstract)
mean (nchar(abstract)/nchar(my_articles$Abstract), na.rm=TRUE)
abstract[nchar(abstract)/nchar(my_articles$Abstract)<0.5]
#Add cleaned abstracts as a NEW column.
#We could also replace the existing but debugging is easier if we keep both.
my_articles$Abstract_clean = tolower(abstract)
my_articles$Title = tolower(my_articles$Title)
#Remove papers that are summaries of conference proceedings.
#If check needed otherwise 0 would remove all papers.
if (length(grep("proceedings contain", my_articles$Abstract_clean, ignore.case = TRUE)) > 0){
my_articles = my_articles[-grep("proceedings contain", my_articles$Abstract_clean, ignore.case = TRUE),]
}
#Date is character convert to Date object
my_articles$Date = as.Date(my_articles$Date)
#Fixed filename: /data/my_scopus_<my_filename>_data.RData
my_file = my_work_dir
my_file = paste(my_file, "/data/my_Scopus_", sep="", collapse=" ")
my_file = paste(my_file, my_filename, sep="", collapse=" ")
my_file = paste(my_file, "_data.RData", sep="", collapse=" ")
save(my_articles, file=my_file)
# my_articles ->  my_articles_SC
# my_articles ->  my_articles_SC
#  return(my_file)
